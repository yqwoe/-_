<!DOCTYPE html><html><head><meta charset="utf-8"><title>Transformer模型 | rm -rf /*</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.ico"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.ico"><link rel="mask-icon" href="{{ url_for(theme.favicon.safari_pinned_tab) }}" color="{{ theme.android_chrome_color }}"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/search.css"><link rel="stylesheet" href="/css/katex.min.css"><link rel="stylesheet" href="/css/googleapis.css"><link rel="stylesheet" href="/css/highlight.css"><meta name="description" content="Transformer模型最早是由Google的研究团队在2017年提出的，并在论文《Attention is All You Need》[1] 中进行了详细的介绍。这个模型是为了解决自然语言处理（NLP）任务中的一些问题而设计的，特别是为了克服传统循环神经网络（RNN）和长短期记忆网络（LSTM）在处理长序列时的局限性，如梯度消失或梯度爆炸问题。   Transformer模型的核心思想是使"><meta property="og:type" content="article"><meta property="og:title" content="Transformer模型"><meta property="og:url" content="https://rm-rf.ink/*/e4b8a22b.html"><meta property="og:site_name" content="rm -rf &#x2F;*"><meta property="og:description" content="Transformer模型最早是由Google的研究团队在2017年提出的，并在论文《Attention is All You Need》[1] 中进行了详细的介绍。这个模型是为了解决自然语言处理（NLP）任务中的一些问题而设计的，特别是为了克服传统循环神经网络（RNN）和长短期记忆网络（LSTM）在处理长序列时的局限性，如梯度消失或梯度爆炸问题。   Transformer模型的核心思想是使"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2024-02-22T03:01:14.000Z"><meta property="article:modified_time" content="2024-02-22T03:30:25.314Z"><meta property="article:author" content="Yq Woe"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="NLP"><meta property="article:tag" content="text2sql"><meta name="twitter:card" content="summary"><link rel="stylesheet" href="/css/reveal.css"><meta name="generator" content="Hexo 7.1.1"><link rel="alternate" href="/atom.xml" title="rm -rf /*" type="application/atom+xml">
</head><body><div id="wrapper"><header id="header"><div class="poem-wrap"><div class="poem-border poem-left"></div><div class="poem-border poem-right"></div><h1><a href="/">rm -rf /*</a></h1><p id="poem">加载中...</p><p id="info"></div><nav><a class="nav-link" href="/">首页</a> <span class="nav-spacer">x</span> <a class="nav-link" href="/categories">分类</a> <span class="nav-spacer">x</span> <a class="nav-link" href="/tags">标签</a> <span class="nav-spacer">x</span> <a class="nav-link" href="/about">关于</a></nav></header><div id="content"><article id="post-Transformer模型" class="article article-type-post" itemprop="blogPost" itemscope><div class="article-inner"><header class="article-header"><h2 class="article-title" itemprop="headline name">Transformer模型</h2><div class="article-meta"><time class="article-date" datetime="2024-02-22T03:01:14.000Z" itemprop="datePublished">2024.02.22 11:01 上午</time></div></header><div class="article-entry" itemprop="articleBody"><p>  Transformer模型最早是由Google的研究团队在2017年提出的，并在论文《Attention is All You Need》<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup> 中进行了详细的介绍。这个模型是为了解决自然语言处理（NLP）任务中的一些问题而设计的，特别是为了克服传统循环神经网络（RNN）和长短期记忆网络（LSTM）在处理长序列时的局限性，如梯度消失或梯度爆炸问题。</p><p>  Transformer模型的核心思想是使用自注意力机制（Self-Attention Mechanism）来捕捉输入序列中的依赖关系。这种机制允许模型在处理每个输入元素时，都能够关注到输入序列中的所有其他元素，从而能够捕捉到更长的依赖关系。此外，Transformer模型还采用了位置编码（Positional Encoding）来弥补模型本身不具备处理序列顺序的能力。</p><p>  Transformer模型由两部分组成：编码器（Encoder）和解码器（Decoder）。编码器负责将输入序列转换为一系列向量表示，而解码器则负责根据这些向量表示生成输出序列。这两部分都由多个相同的层堆叠而成，每一层都包含一个多头自注意力子层和一个简单的全连接前馈神经网络。</p><p>  由于Transformer模型不依赖RNN的顺序结构，因此它可以实现高效的并行化训练，这使得它在处理大规模数据集时具有显著的优势。此外，Transformer模型的强大表示能力和灵活性使得它在多个NLP任务上都取得了显著的效果，成为了自然语言处理领域的一种主流模型。</p><p>  总的来说，Transformer模型是Google研究团队在2017年提出的一种基于自注意力机制的深度学习模型，它的出现为自然语言处理领域的发展带来了革命性的突破。</p><p>引用:</p><ul class="lvl-0"><li class="lvl-2"><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/647249972">Text-to-SQL小白入门（一）</a></p></li><li class="lvl-2"><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/650407036">Text-to-SQL小白入门（二）</a></p></li></ul><p>原论文:</p><div class="row"><embed src="/images/1706.03762.pdf" width="100%" height="550" type="application/pdf"></div><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>《Attention is All You Need》 <a href="#fnref1" class="footnote-backref">↩︎</a></p></li></ol></section></div><div class="article-category"><a class="article-category-link" href="/categories/%E5%AD%A6%E4%B9%A0/">学习</a> <b>&nbsp;:分类</b><br><a class="article-tag-none-link" href="/tags/NLP/" rel="tag">NLP</a>, <a class="article-tag-none-link" href="/tags/text2sql/" rel="tag">text2sql</a>, <a class="article-tag-none-link" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag">机器学习</a> <b>&nbsp;:标签</b></div></div></article><nav id="article-nav" class="article-nav"><span id="article-nav-newer" class="article-nav-link-wrap newer"></span> <a href="/*/c88b93d0.html" id="article-nav-older" class="article-nav-link-wrap older"><strong class="article-nav-caption">下一篇</strong><div class="article-nav-title">笔记4-《程序员数学-1》</div></a></nav><div id="gitalk-container" style="padding:0 30px 0 30px"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script><script type="text/javascript">var gitalk=new Gitalk({clientID:"d99a4bdc344cff33585d",clientSecret:"4819ca8aa103e688f259b5ed6b3cd3fffdf0f96e",repo:"yqwoe.github.io",owner:"yqwoe",admin:["yqwoe"],perPage:20,id:"e4b8a22b",distractionFreeMode:"true",pagerDirection:"last"});gitalk.render("gitalk-container")</script></div></div><div id="settings-container"><div id="dark-mode"></div><div id="sans-font"></div></div><span class="post-count">23.7k</span><script type="text/javascript">let d=document,r=d.documentElement.style,f=r.setProperty.bind(r),l=localStorage,s=l.getItem("s")||window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,n=l.getItem("n"),m=d.getElementById("dark-mode"),b=()=>{f("--bg-color","#fafafa"),f("--code-bg-color","#f4f4f4"),f("--text-color","#212121"),f("--secondary-color","#808080"),f("--tertiary-color","#b0b0b0"),f("--link-color","#b5c8cf"),f("--link-hover-color","#618794"),f("--link-bg-color","#dae4e7"),f("--selection-color","#dae4e7")},c=()=>{f("--bg-color","#212121"),f("--code-bg-color","#292929"),f("--text-color","#fff"),f("--secondary-color","#c0c0c0"),f("--tertiary-color","#6e6e6e"),f("--link-color","#4d6b75"),f("--link-hover-color","#96b1bb"),f("--link-bg-color","#5d828e"),f("--selection-color","#acc1c9")},o=d.getElementById("sans-font"),e=()=>{f("--body-stack",'"Lora", "Georgia", "Times New Roman", serif')},g=()=>{f("--body-stack",'"Lato", "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", "Verdana", sans-serif')};m.onclick=function(){2==s?(s=1,l.setItem("s",s),c()):(s=2,l.setItem("s",s),b())},o.onclick=function(){2==n?(n=1,l.setItem("n",n),g()):(n=2,l.setItem("n",n),e())},(s||"1"==s)&&(s=1,l.setItem("s",s),c()),s&&"2"!=s||(s=2,l.setItem("s",s),b()),n||(n=2,l.setItem("n",2)),"1"==n&&g(),"2"==n&&e()</script><script type="module">import mermaid from"https://cdn.jsdelivr.net/npm/mermaid@9.4.3/+esm";let theme=localStorage.getItem("s");mermaid.initialize({startOnLoad:!0,logLevel:"debug",theme:"2"===theme?"forest":"dark"});let mode=document.getElementById("dark-mode"),arrayGraphs=[],nodes=document.getElementsByClassName("mermaid");for(let e of nodes)console.log(e.innerHTML),arrayGraphs.push(e.innerHTML);mode.addEventListener("click",async()=>{theme=localStorage.getItem("s"),mermaid.initialize({startOnLoad:!0,logLevel:"debug",theme:"2"===theme?"forest":"dark"});for(let e=0;e<nodes.length;e++){let t=nodes[e];t.removeAttribute("data-processed"),t.setAttribute("id","mermaid-"+e),t.replaceChildren(),t.innerHTML=arrayGraphs[e],mermaid.init(void 0,t)}},!1)</script><script src="/js/jquery.min.js" defer></script><script src="/js/jquery.qrcode.min.js" defer></script><script type="module">import qrcodeInit from"/js/qrcode.js";qrcodeInit()</script><script type="module">window.requestAnimationFrame=window.requestAnimationFrame||window.webkitRequestAnimationFrame||window.mozRequestAnimationFrame||window.oRequestAnimationFrame||window.msRequestAnimationFrame||function(n){window.setTimeout(n,100)},window.cancelAnimationFrame=window.cancelAnimationFrame||window.webkitCancelAnimationFrame||window.mozCancelAnimationFrame||window.oCancelAnimationFrame||window.msCancelAnimationFrame||clearTimeout</script><script type="module">function get_poem(n,e){var t=document.querySelector(n),o=document.querySelector(e),i=new XMLHttpRequest;i.open("get","https://v2.jinrishici.com/one.json?client=browser-sdk/1.2"),i.withCredentials=!0,i.onreadystatechange=function(){if(4===i.readyState){var n=JSON.parse(i.responseText);t.innerHTML=n.data.content,o.innerHTML="["+n.data.origin.dynasty+"] "+n.data.origin.author+"《"+n.data.origin.title+"》"}},i.send()}function modifySubmitBtn(){var n=$(".hbe-button"),e=$("#hbePass");n.length>0&&n.text("重新输入密码"),e.length>0&&e.attr("autocomplete","false")}function layoutTagCloud(){}function loopHover(){var n=$(".tagcloud a"),e=$('.tagcloud a[class="tag-hover"]'),t=Math.round(Math.random()*(n.length-1));if(t<0&&(t=0),t>n.length&&(t=n.length-1),n.length){if(e.length)for(let n=0;n<e.length;n++)e[n].classList="";n[t].classList="tag-hover"}}function setIntervalPrecision(n,e){let t=window.interValPrecisionObj||(window.interValPrecisionObj={num:0});t.num++,t["n"+t.num]=!0;var o=t.num,i=+new Date,a=0;return e=e||0,function r(){t["n"+o]&&(+new Date>i+e*(a+1)&&(a++,n(a)),window.requestAnimationFrame(r))}(),o}function clearIntervalPrecision(n){window.interValPrecisionObj&&(window.cancelAnimationFrame("n"+n),delete window.interValPrecisionObj["n"+n])}$("div").hasClass("poem-wrap")&&get_poem("#poem","#info"),modifySubmitBtn(),setIntervalPrecision(loopHover,100)</script></body></html>